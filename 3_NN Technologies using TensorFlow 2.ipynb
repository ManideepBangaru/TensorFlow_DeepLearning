{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_items = 11\n",
    "num_list1 = np.arange(num_items)\n",
    "num_list2 = np.arange(num_items,num_items*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is how to create datasets, using the from_tensor_slices() method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is how to create an iterator on it using the make_one_shot_iterator() method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.compat.v1.data.make_one_shot_iterator(num_list1_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for item in num_list1_dataset:\n",
    "    num = iterator.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that executing this code twice in the same program run will raise an error because we are using a one-shot iterator\n",
    "\n",
    "#### It is also possible to access the data in batches with batch method. Note that first argument is the number of elements to put in each batch and the second is the self-explanatory drop_remainder argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list1_dataset = tf.data.Dataset.from_tensor_slices(num_list1).batch(3,drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.compat.v1.data.make_one_shot_iterator(num_list1_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n",
      "[3 4 5]\n",
      "[6 7 8]\n",
      "[ 9 10]\n"
     ]
    }
   ],
   "source": [
    "for item in num_list1_dataset:\n",
    "    num = iterator.get_next().numpy()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is also a zip method, which is useful for presenting features and labels together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=string, numpy=b'a'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=string, numpy=b'e'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=string, numpy=b'i'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=4>, <tf.Tensor: shape=(), dtype=string, numpy=b'o'>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=5>, <tf.Tensor: shape=(), dtype=string, numpy=b'u'>)\n"
     ]
    }
   ],
   "source": [
    "dataset1 = [1,2,3,4,5]\n",
    "dataset2 = ['a','e','i','o','u']\n",
    "dataset1 = tf.data.Dataset.from_tensor_slices(dataset1)\n",
    "dataset2 = tf.data.Dataset.from_tensor_slices(dataset2)\n",
    "zipped_datasets = tf.data.Dataset.zip((dataset1,dataset2))\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(zipped_datasets)\n",
    "for item in zipped_datasets:\n",
    "    num = iterator.get_next()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can concatenate two datasets as following, using the concatenate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConcatenateDataset shapes: (), types: tf.int32>\n"
     ]
    }
   ],
   "source": [
    "ds1 = tf.data.Dataset.from_tensor_slices([1,2,3,4,5,6,7,8,9,10])\n",
    "ds2 = tf.data.Dataset.from_tensor_slices([11,12,13,14,15,16,17,18,19,20])\n",
    "ds3 = ds1.concatenate(ds2)\n",
    "print(ds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = tf.compat.v1.data.make_one_shot_iterator(ds3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(14):\n",
    "    num = iterator.get_next()\n",
    "    print(num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do away with iterators altogether, as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(15, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(4, shape=(), dtype=int32)\n",
      "tf.Tensor(5, shape=(), dtype=int32)\n",
      "tf.Tensor(6, shape=(), dtype=int32)\n",
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(8, shape=(), dtype=int32)\n",
      "tf.Tensor(9, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(11, shape=(), dtype=int32)\n",
      "tf.Tensor(12, shape=(), dtype=int32)\n",
      "tf.Tensor(13, shape=(), dtype=int32)\n",
      "tf.Tensor(14, shape=(), dtype=int32)\n",
      "tf.Tensor(15, shape=(), dtype=int32)\n",
      "tf.Tensor(16, shape=(), dtype=int32)\n",
      "tf.Tensor(17, shape=(), dtype=int32)\n",
      "tf.Tensor(18, shape=(), dtype=int32)\n",
      "tf.Tensor(19, shape=(), dtype=int32)\n",
      "tf.Tensor(20, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for e in range(epochs):\n",
    "    for item in ds3:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using comma-separated value (CSV) files with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSV files are a very popular method of storing data. TensorFlow 2 contains flexible methods for dealing with them. The main method here is\n",
    "\n",
    "tf.data.experimental.CsvDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following arguments, our dataset will consist of two items taken from each row of the filename file, both of the float type, with the first line of the file ignored and columns 1 and 2 used (column numbering is, of course, 0-based):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = ['/size_1000.csv']\n",
    "record_defaults = [tf.float32] * 2 # Two required float columns\n",
    "dataset = tf.data.experimental.CsvDataset(filename,record_defaults,header = True, select_cols = [1,2])\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, and with the following arguments, our dataset will consist of one required float, one optional float with a default value of 0.0, and an int, where there is no header in the CSV file and only columns 1, 2, and 3 are imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mycsvfile.txt'\n",
    "record_defaults = [tf.float32,tf.constant([0.0],dtype = tf.float32),tf.int32]\n",
    "dataset = tf.data.experimental.CsvDataset(filename,record_defaults,header = False, select_cols = [1,2,3])\n",
    "for item in dataset:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our final example, our dataset will consist of two required floats and a required string, where the CSV file has a header variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'file1.txt'\n",
    "record_defaults = [tf.float32,tf.float32,tf.string,]\n",
    "dataset = tf.data.experimental.CsvDataset(filename,record_defaults,header=False)\n",
    "for item in dataset:\n",
    "    print(item[0].numpy(), item[1].numpy, item[2].numpy().decode()) # decode as string is in binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular choice for storing data is the TFRecord format. This is a binary file format. For large files, it is a good choice because binary files take up less disc space, take less time to copy, and can be read very efficiently from the disc. All this can have a significant effect on the efficiency of your data pipeline and, thus, the training time of your model. The format is also optimized in a variety of ways for use with TensorFlow. It is a little complex because data has to be converted into the binary format prior to storage and decoded when read back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a TFRecord file is a sequence of binary strings, its structure must be specified prior to saving so that it can be properly written and subsequently read back. TensorFlow has two structures for this, tf.train.Example and tf.train.SequenceExample. What you have to do is store each sample of your data in one of these structures, then serialize it, and use tf.python_io.TFRecordWriter to save it to disk.\n",
    "\n",
    "In the following example, the float array, data, is converted to the binary format and then saved to disc. A feature is a dictionary containing the data that is passed to tf.train.Example prior to serialization and saving. A more elaborate example of this is shown in TFRecord example 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([10.,11.,12.,13.,14.,15.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npy_to_tfrecords(fname,data):\n",
    "    writer = tf.io.TFRecordWriter(fname)\n",
    "    feature = {}\n",
    "    feature['data'] = tf.train.Feature(float_list=tf.train.FloatList(value = data))\n",
    "    example = tf.train.Example(features=tf.train.Features(feature = Features))\n",
    "    serialized = example.SerializeToString()\n",
    "    writer.write(serialized)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to read the record back is as follows. A parse_function function is constructed that decodes the dataset read back from the file. This requires a dictionary (keys_to_features) with the same name and structure as the saved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset('./myfile.tfrecords')\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'data' : tf.io.FixedLenSequenceFeature([], dtype = tf.float32, allow_missing = True)}\n",
    "    parsed_features = tf.io.parse_single_example(serialized = example_proto, features = keys_to_features)\n",
    "    return parsed_geatures['data']\n",
    "\n",
    "dataset = dataset.map(parse_function)\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "\n",
    "# array is retrieved as one item\n",
    "item = iterator.get_next()\n",
    "\n",
    "print(item)\n",
    "print(item.numpy())\n",
    "print(item[2].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFRecord example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we look at a more complicated record structure given by this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './students.tfrecords'\n",
    "data = {\n",
    "    'ID'     : 61553,\n",
    "    'Name'   : ['Jones','Felicity'],\n",
    "    'Scores' : [45.6 , 97.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this, we can construct a tf.train.Example class, again using the Feature( ) method.\n",
    "Notw how we have to encode our string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = tf.train.Feature(int64_list = tf.train.Int64List(value = [data['ID']]))\n",
    "Name = tf.train.Feature(bytes_list=tf.train.BytesList(value=[n.encode('utf-8') for n in data['Name']]))\n",
    "Scores = tf.train.Feature(float_list = tf.train.FloatList(value = data['Scores']))\n",
    "\n",
    "example = tf.train.Example(features = tf.train.Features(feature = {'ID' : ID, 'Name' : Name, 'Scores' : Scores}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serializing and writing this recored to disc is the same as TFRecord example 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.io.TFRecordWriter('/Users/manideepbangaru/Documents/TensorFlow_DeepLearning-master/Students.tfrecords')\n",
    "writer.write(example.SerializeToString())\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read this back, we just need to construct our parse_function to reflect the structure of the record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(\"/Users/manideepbangaru/Documents/TensorFlow_DeepLearning-master/Students.tfrecords\")\n",
    "\n",
    "def parse_function(example_proto):\n",
    "    keys_to_features = {'ID' : tf.io.FixedLenFeature([],dtype = tf.int64),\n",
    "                       'Name' : tf.io.VarLenFeature(dtype = tf.string),\n",
    "                       'Scores' : tf.io.VarLenFeature(dtype = tf.float32)\n",
    "                       }\n",
    "    parsed_features = tf.io.parse_single_example(serialized = example_proto,\n",
    "                                                features = keys_to_features)\n",
    "    return parsed_features['ID'], parsed_features['Name'], parsed_features['Scores']\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int64, numpy=61553>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x151c4b6d0>, <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x151cbcb50>)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(parse_function)\n",
    "\n",
    "iterator = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "item = iterator.get_next()\n",
    "\n",
    "# record is retrieved as one item\n",
    "print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can extract our data from item (note that the string must be decoded (from bytes) where the default for our Python 3 is utf8). Note also that the string and the array of floats are returned as sparse arrays, and to extract them from the record, we use the sparse array value method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID :  61553\n",
      "Name: Jones , Felicity\n",
      "Scores:  [45.6 97.2]\n"
     ]
    }
   ],
   "source": [
    "print('ID : ',item[0].numpy())\n",
    "name = item[1].values.numpy()\n",
    "name1 = name[0].decode()\n",
    "name2 = name[1].decode('utf8')\n",
    "print('Name:',name1,\",\",name2)\n",
    "print(\"Scores: \",item[2].values.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-hot encoding (OHE) is where a tensor is constructed from the data labels with a 1 in each of the elements corresponding to a label's value, and 0 everywhere else; that is, one of the bits in the tensor is hot (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are converting a decimal value of 5 to a one-hot encoded value of 0000100000 using the tf.one_hot() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 is [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] when one-hot encoded with a depth of 10\n"
     ]
    }
   ],
   "source": [
    "y = 5\n",
    "y_train_ohe = tf.one_hot(y,depth = 10).numpy()\n",
    "print(y,'is',y_train_ohe,'when one-hot encoded with a depth of 10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OHE example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also nicely shown in the following example using the sample code that imports from the fashion MNIST dataset.\n",
    "\n",
    "The original labels are integers from 0 to 9, so, for example, a label of 2 becomes 0010000000 when one-hot encoded, but note the difference between the index and the label stored at that index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "width,height, = 28,28\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
    "\n",
    "split = 5000\n",
    "# split feature training set into training and validation sets\n",
    "(y_train,y_valid) = y_train[:split],y_train[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the labels using TensorFlow\n",
    "# then convert back to numpy for display\n",
    "y_train_ohe = tf.one_hot(y_train,depth = n_classes).numpy()\n",
    "y_valid_ohe = tf.one_hot(y_valid,depth = n_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test,depth = n_classes).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show the difference between original label and one hot encoded label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "i = 5\n",
    "print(y_train[i]) # 'ordinary' number value of label at index i=5 is 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# note the difference between the index of 5 and the label at that index which is 2\n",
    "print(y_train_ohe[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
